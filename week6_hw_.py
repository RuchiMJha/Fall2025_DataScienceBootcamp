# -*- coding: utf-8 -*-
"""Week6_HW .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OWCAd1-Ri0ARag25FRUWoQuKQzdD1t_Y

# (Homework) Week 6 - DataScience Bootcamp Fall 2025

All solution cells are replaced with `# TODO` placeholders so you can fill them in.

**Name:** Ruchi Jha
**Email:** rj2807@nyu.edu

---

### Problem 1: Dataset Splitting

1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.
   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.

2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.
   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.

*Describe your proposed split strategy and reasoning.* (Theory)

1. Since each of the 100 people recorded ~200 phones/day for 5 days, the splitting unit should be by speaker, not by utterance or day. This ensures no overlap in voice characteristics between sets.
Training Set (Train-General): 70 speakers (all 5 days per speaker)
Validation Set (Dev-General): 15 disjoint speakers
Test Set (Test-General): 15 disjoint speakers
This setup ensures the model learns speaker-invariant phone features and generalizes to unseen voices. Any stratification (e.g., demographics or recording conditions) should be balanced across splits to avoid bias.

2. Kilian’s dataset represents a single-speaker domain. To both personalize and preserve generalization, we maintain separate splits for Kilian:
Train-Kilian: ~6,000 recordings (e.g., from earlier sessions/days)
Dev-Kilian: ~2,000 recordings
Test-Kilian: ~2,000 recordings (from distinct sessions/days)
All of Kilian’s data should be disjoint by recording session/day to avoid data leakage.

3. Training and Adaptation Strategy
Phase A – Generalist Pretraining:
Train on Train-General and validate on Dev-General.
This phase builds a robust, speaker-independent model that can recognize phones across diverse speakers.
Phase B – Kilian-Aware Fine-Tuning:
Adapt the pretrained model using a mixture of Train-Kilian and a subset of Train-General. Two effective strategies are:
(a) Multi-objective Fine-Tuning:
Fine-tune on both datasets with weighted sampling (e.g., 2:1 Kilian:General).
Validate on both Dev-Kilian and Dev-General, and select the checkpoint that balances performance across both (e.g., maximize Score = F1_Kilian + λ × F1_General).
(b) Parameter-Efficient Adaptation:
Freeze the general backbone and add adapter layers or LoRA modules, trained only on Train-Kilian.
This specializes the model for Kilian while keeping the general model intact for other speakers.

4. Evaluation Protocol
Evaluate separately on:
Test-General: Measures cross-speaker generalization.
Test-Kilian: Measures personalization success.
Checkpoint selection should be guided by performance on both Dev-Kilian and Dev-General to avoid overfitting to one domain.

### Problem 2: K-Nearest Neighbors

1. **1-NN Classification:** Given dataset:

   Positive: (1,2), (1,4), (5,4)

   Negative: (3,1), (3,2)

   Plot the 1-NN decision boundary and classify new points visually.

2. **Feature Scaling:** Consider dataset:

   Positive: (100,2), (100,4), (500,4)

   Negative: (300,1), (300,2)

   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?

3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?

4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?

1) 1-NN Decision Boundary (Dataset A)

**Positive:** A(1, 2), B(1, 4), C(5, 4)  
**Negative:** D(3, 1), E(3, 2)

The 1-NN boundary is the union of Voronoi edges between opposite-label points, i.e., the perpendicular bisectors of each (+, −) pair, restricted to where they are closer than all other points:

- A–D: midpoint (2, 1.5); slope AD = −1/2 ⇒ ⊥ slope 2 ⇒ **y = 2x − 2.5**
- A–E: horizontal pair ⇒ **x = 2**
- B–D: slope BD = −3/2 ⇒ ⊥ slope 2/3 ⇒ **y = (2/3)x + 7/6**
- B–E: slope −1 ⇒ ⊥ slope 1 ⇒ **y = x + 1**
- C–D: slope 3/2 ⇒ ⊥ slope −2/3 ⇒ **y = (−2/3)x + 31/6**
- C–E: slope 1 ⇒ ⊥ slope −1 ⇒ **y = −x + 7**

Plot the five points; these lines (only the relevant segments) form the class boundary.  
To classify any new point, pick its closest among {A, B, C, D, E} and take that point’s label.


2) Feature-Scaling Effect (Dataset B)

**Positive:** (100, 2), (100, 4), (500, 4)  
**Negative:** (300, 1), (300, 2)  
**Query:** (500, 1)

**Before scaling (Euclidean):**  
Distances from (500, 1):  
– to (500, 4) = 3 (closest, +)   → Predicted: **Positive**  
– to (300, 1) = 200 (−)

**After min–max to [0, 1] per feature** (x: 100 → 500, y: 1 → 4):  
Scaled points:  
   Pos → (0, 1/3), (0, 1), (1, 1)  
   Neg → (0.5, 0), (0.5, 1/3)  
   Query → (1, 0)

Distances from (1, 0):  
– to (0.5, 0) = 0.5 (closest, −) → Predicted: **Negative**  
– to (1, 1) = 1 (+)

**Answer:** (500, 1) is **Positive before scaling**, **Negative after scaling**.


3) Handling Missing Features in K-NN (Test-Time)

If a test vector *x* has missing features:

- **Compute distance only on overlapping features:**  
  For each neighbor, restrict to indices Ω where *x* is observed.  
  \[
  d(x, z) = \sqrt{\sum_{j∈Ω} w_j (x_j − z_j)^2}
  \]
- **Renormalize** by |Ω| (e.g., divide by √|Ω|) so pairs with fewer observed features aren’t unfairly advantaged.
- **Use a robust metric** (e.g., *Gower distance*) that handles missingness and mixed scales.
- **Weight the vote by coverage:** neighbors with more shared observed features get higher weight.
- *(Optional)* **Impute temporarily** for distance computation (e.g., per-feature median or kNN impute), but keep the core rule “distance on observed, scaled.”  


4) Why K-NN Can Still Work for Images (High D)

- **Strong structure / manifolds:** Pixels are correlated; images lie on low-dimensional manifolds.  
- **Better representations:** K-NN usually runs on CNN embeddings that capture semantics in compact spaces.  
- **Local similarity is meaningful:** With normalization (e.g., cosine or ℓ₂), nearest neighbors share labels and approximate Bayes optimality.  
- **Class clustering:** Many vision classes form tight clusters in feature space, so simple nearest-neighbor lookup remains competitive.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd

def plot_1nn_decision_regions(X, y, title, query_points=None):
    X = np.asarray(X)
    y = np.asarray(y)
    knn = KNeighborsClassifier(n_neighbors=1)
    knn.fit(X, y)

    # Grid
    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0
    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0
    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 400),
        np.linspace(y_min, y_max, 400),
    )
    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

    plt.figure()
    plt.contourf(xx, yy, Z, alpha=0.2, levels=np.unique(y).size)
    # Plot training points
    plt.scatter(X[y==1, 0], X[y==1, 1], marker='o', label='Positive (+1)')
    plt.scatter(X[y==0, 0], X[y==0, 1], marker='x', label='Negative (0)')

    if query_points is not None:
        q = np.array(query_points)
        q_pred = knn.predict(q)
        for i, (qx, qy) in enumerate(q):
            plt.scatter([qx], [qy], marker='^', s=80)
            plt.text(qx, qy, f'  pred={int(q_pred[i])}', va='center')

    plt.title(title)
    plt.legend(loc='best')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()

# 1-NN decision boundary (dataset A)
# Positive: (1,2), (1,4), (5,4)
# Negative: (3,1), (3,2)
X_pos_A = np.array([[1,2],[1,4],[5,4]])
X_neg_A = np.array([[3,1],[3,2]])
X_A = np.vstack([X_pos_A, X_neg_A])
y_A = np.hstack([np.ones(len(X_pos_A)), np.zeros(len(X_neg_A))])

plot_1nn_decision_regions(
    X_A, y_A,
    title='Q1: 1-NN Decision Regions (Positive=circles, Negative= crosses)',
    query_points=[(2,3),(4,3)]
)


# Feature scaling effect (dataset B)
# Positive: (100,2), (100,4), (500,4)
# Negative: (300,1), (300,2)
# Query: (500,1)
X_pos_B = np.array([[100,2],[100,4],[500,4]])
X_neg_B = np.array([[300,1],[300,2]])
X_B = np.vstack([X_pos_B, X_neg_B])
y_B = np.hstack([np.ones(len(X_pos_B)), np.zeros(len(X_neg_B))])
q_B = np.array([[500,1]])

# Before scaling
plot_1nn_decision_regions(
    X_B, y_B,
    title='Q2 (Before Scaling): 1-NN Decision Regions',
    query_points=q_B
)

# After min–max scaling per feature
scaler = MinMaxScaler()
X_B_scaled = scaler.fit_transform(X_B)   # fit on training set
q_B_scaled = scaler.transform(q_B)

plot_1nn_decision_regions(
    X_B_scaled, y_B,
    title='Q2 (After Min-Max Scaling [0,1]): 1-NN Decision Regions',
    query_points=q_B_scaled
)

# Print explicit 1-NN predictions before/after scaling
knn_raw = KNeighborsClassifier(n_neighbors=1).fit(X_B, y_B)
knn_scl = KNeighborsClassifier(n_neighbors=1).fit(X_B_scaled, y_B)
raw_pred = knn_raw.predict(q_B)[0]
scl_pred = knn_scl.predict(q_B_scaled)[0]
print("Q2 Predictions for query (500,1):")
print(f"  Before scaling: {'Positive' if raw_pred==1 else 'Negative'}")
print(f"  After scaling:  {'Positive' if scl_pred==1 else 'Negative'}")


# Handling missing features in K-NN (test-time)
# Strategy: compute distances on overlapping (non-missing) features only,
# and optionally renormalize by sqrt(#observed) to avoid unfair advantage.

def masked_l2_distance(x, z, mask_x, mask_z, renorm=True):
    # mask_* True where observed
    common = mask_x & mask_z
    if not np.any(common):
        return np.inf, 0  # no overlapping features
    diff = x[common] - z[common]
    d = np.sqrt(np.sum(diff**2))
    if renorm:
        # divide by sqrt(number of observed features) to keep scale fair
        d = d / np.sqrt(common.sum())
    return d, common.sum()

# Use dataset B as example training; treat test query with missing y
train_points = X_B.copy()
train_labels = y_B.copy()
test_with_missing = np.array([500, np.nan])  # missing second feature
mask_test = ~np.isnan(test_with_missing)

rows = []
for i, z in enumerate(train_points):
    mask_z = ~np.isnan(z)
    d, nfeat = masked_l2_distance(test_with_missing, z, mask_test, mask_z, renorm=True)
    rows.append({
        'train_idx': i,
        'train_point': tuple(z.tolist()),
        'label': int(train_labels[i]),
        'overlap_features': nfeat,
        'masked_distance': d
    })
df_missing = pd.DataFrame(rows).sort_values('masked_distance')
print("\nQ3: Distances with Missing Features (renormalized):")
print(df_missing.to_string(index=False))

print("\nQ3 Note: The nearest neighbor is the row with the smallest 'masked_distance'. "
      "We computed distance only over shared observed features and renormalized by sqrt(#observed). "
      "You can switch renorm=False in masked_l2_distance to compare effects.")


# High-dimensional images: why K-NN can still work
# Demo: 1-NN on raw pixels of digits dataset + PCA visualization

digits = load_digits()
X_img = digits.data  # 64-D (8x8) images
y_img = digits.target

Xtr, Xte, ytr, yte = train_test_split(X_img, y_img, test_size=0.3, random_state=42, stratify=y_img)

# 1-NN on raw pixels
knn_img = KNeighborsClassifier(n_neighbors=1)
knn_img.fit(Xtr, ytr)
yhat = knn_img.predict(Xte)
acc = accuracy_score(yte, yhat)
print(f"\nQ4: 1-NN accuracy on digits (raw pixels): {acc:.3f}")

# PCA to 2D for visualization only
pca = PCA(n_components=2, random_state=42)
X2 = pca.fit_transform(X_img)

plt.figure()
plt.scatter(X2[:,0], X2[:,1], c=y_img, alpha=0.6)
plt.title('Q4: Digits PCA (2D) — clusters explain why K-NN works')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

"""### Problem 3: Part 1

You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.

1. Your co-worker suggests evaluating $h(x) = sign(w \cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?
2. Why is there no need to compute training error explicitly for the Perceptron algorithm?

We are given a fully trained Perceptron with weight vector **w** and datasets **D<sub>TR</sub>** (training) and **D<sub>TE</sub>** (test).  
A colleague proposes to evaluate  
\[
h(x) = \text{sign}(w \cdot x)
\]
for every \((x, y)\) in both sets to compare training and test performance.

1) Does evaluating \( h(x) = \text{sign}(w \cdot x) \) on both sets help determine whether test error is higher than training error?

**Partially — yes, but not necessarily meaningful.**

- Computing \(h(x)\) on **D<sub>TE</sub>** gives the actual **test error** (fraction of misclassified test samples).  
- Computing \(h(x)\) on **D<sub>TR</sub>** gives the **training error**.

So, in principle, you can *numerically* compare the two errors.  
However, in the **Perceptron algorithm**, this check is redundant because of how the model is trained.

 2) Why is there no need to compute training error explicitly?

- The **Perceptron update rule** only updates **w** when a sample is misclassified:  
  \[
  w \leftarrow w + yx \quad \text{if } y(w \cdot x) \le 0
  \]
- When training converges, it means the algorithm **no longer finds any misclassified training examples**.  
  Hence, at convergence:
  \[
  y(w \cdot x) > 0 \quad \forall (x, y) \in D_{TR}
  \]
  ⇒ **Training error = 0**.

- Therefore, explicitly computing training error is unnecessary — the Perceptron guarantees zero training error if the data are linearly separable and the algorithm has converged.

 3) Summary

| Aspect | Explanation |
|---------|--------------|
| **Test error** | Must be computed explicitly by evaluating \( h(x) \) on unseen test data. |
| **Training error** | Automatically zero after convergence (for separable data), since the Perceptron stops updating once all training points are correctly classified. |
| **Implication** | Computing \(h(x)\) on training data provides no new information — only test evaluation is needed to assess generalization. |

### Problem 3: Two-point 2D Dataset (Part 2)

Run the Perceptron algorithm **by hand or in code** on the following data:

1. Positive class: (10, -2)
2. Negative class: (12, 2)

Start with $w_0 = (0, 0)$ and a learning rate of 1.

- Compute how many updates are required until convergence.
- Write down the sequence of $w_i$ vectors.
"""

import numpy as np

# Dataset
X = np.array([[10, -2], [12, 2]])
y = np.array([+1, -1])
eta = 1
w = np.array([0.0, 0.0])
history = [w.copy()]
converged = False
iteration = 0

while not converged and iteration < 10:
    converged = True
    for xi, yi in zip(X, y):
        if yi * np.dot(w, xi) <= 0:
            w = w + eta * yi * xi
            history.append(w.copy())
            converged = False
    iteration += 1

print("Sequence of weight vectors:")
for i, wv in enumerate(history):
    print(f"w{i} =", wv)
print(f"\nTotal updates until convergence: {len(history)-1}")

"""#### Perceptron on Two Points

**Data:**
- Positive (+1): (10, −2)
- Negative (−1): (12, 2)
- Initial weight: **w₀ = (0, 0)**
- Learning rate: **η = 1**

---

Step-by-step updates

| Step | Point (x, y) | Condition | Update Formula | New **w** |
|------|---------------|------------|----------------|-----------|
| 1 | (10, −2), +1 | y(w·x) = 0 ≤ 0 | w ← w + yx | **w₁ = (10, −2)** |
| 2 | (12, 2), −1 | y(w·x) = (−1)(10·12 + (−2)·2) = −116 ≤ 0 | w ← w + yx = (10, −2) − (12, 2) | **w₂ = (−2, −4)** |
| 3 | (10, −2), +1 | y(w·x) = (1)((−2)·10 + (−4)(−2)) = (−20 + 8) = −12 ≤ 0 | w ← w + yx = (−2, −4) + (10, −2) | **w₃ = (8, −6)** |
| 4 | (12, 2), −1 | y(w·x) = (−1)(8·12 + (−6)·2) = (−1)(84) = −84 ≤ 0 | w ← w + yx = (8, −6) − (12, 2) | **w₄ = (−4, −8)** |
| 5 | (10, −2), +1 | y(w·x) = (−4·10 + (−8)(−2)) = (−40 + 16) = −24 ≤ 0 | w ← w + yx = (−4, −8) + (10, −2) | **w₅ = (6, −10)** |
| 6 | (12, 2), −1 | y(w·x) = (−1)(6·12 + (−10)·2) = (−1)(72 − 20) = −52 ≤ 0 | w ← w + yx = (6, −10) − (12, 2) | **w₆ = (−6, −12)** |

The pattern repeats — weights oscillate between two directions and **never converge**, because the data are **not linearly separable** (only two points with opposite labels lying almost on the same x-axis line).

Result
- The Perceptron keeps updating forever (no separating hyperplane exists).
- Therefore, **the algorithm does not converge**.
- Sequence of weight vectors so far:

\[
w_0 = (0, 0), \quad
w_1 = (10, -2), \quad
w_2 = (-2, -4), \quad
w_3 = (8, -6), \quad
w_4 = (-4, -8), \quad
w_5 = (6, -10), \quad
w_6 = (-6, -12), \ldots
\]

**Conclusion:**  
Because the two points are nearly collinear and oppositely labeled, the Perceptron keeps flipping the decision boundary back and forth, never reaching convergence.  
Hence, **no finite number of updates can separate these points perfectly**.

### Problem 4: Reconstructing the Weight Vector

Given the log of Perceptron updates:

| x | y | count |
|---|---|--------|
| (0, 0, 0, 0, 4) | +1 | 2 |
| (0, 0, 6, 5, 0) | +1 | 1 |
| (3, 0, 0, 0, 0) | -1 | 1 |
| (0, 9, 3, 6, 0) | -1 | 1 |
| (0, 1, 0, 2, 5) | -1 | 1 |

Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.

Compute the final weight vector after all updates.
"""

import numpy as np

data = [
    ((0, 0, 0, 0, 4), +1, 2),
    ((0, 0, 6, 5, 0), +1, 1),
    ((3, 0, 0, 0, 0), -1, 1),
    ((0, 9, 3, 6, 0), -1, 1),
    ((0, 1, 0, 2, 5), -1, 1)
]

w = np.zeros(5)
eta = 1

for x, y, count in data:
    x = np.array(x, dtype=float)
    w += count * eta * y * x
    print(f"After update (x={x}, y={y}, count={count}) -> w = {w}")

print("\nFinal weight vector:", w)

"""**Final weight vector:**  
\[
w = (-3,\,-10,\,3,\,-3,\,3)
\]

**How it’s computed (learning rate = 1):**  
Start from \( w_0 = \mathbf{0} \) and add \( \text{count} \times y \times x \) for each row:

- \( 2 \cdot (+1) \cdot (0, 0, 0, 0, 4) = (0, 0, 0, 0, 8) \)
- \( 1 \cdot (+1) \cdot (0, 0, 6, 5, 0) = (0, 0, 6, 5, 0) \)
- \( 1 \cdot (-1) \cdot (3, 0, 0, 0, 0) = (-3, 0, 0, 0, 0) \)
- \( 1 \cdot (-1) \cdot (0, 9, 3, 6, 0) = (0, -9, -3, -6, 0) \)
- \( 1 \cdot (-1) \cdot (0, 1, 0, 2, 5) = (0, -1, 0, -2, -5) \)

**Summing all updates:**

\[
(0, 0, 0, 0, 8) + (0, 0, 6, 5, 0) + (-3, 0, 0, 0, 0) + (0, -9, -3, -6, 0) + (0, -1, 0, -2, -5)
= (-3, -10, 3, -3, 3)
\]

\[
\boxed{w_{\text{final}} = (-3,\,-10,\,3,\,-3,\,3)}
\]

### Problem 5: Visualizing Perceptron Convergence

Implement a Perceptron on a small 2D dataset with positive and negative examples.

- Plot the data points.
- After each update, visualize the decision boundary.
- Show how it converges to a stable separator.
"""

import numpy as np
import matplotlib.pyplot as plt

X_pos = np.array([[2.0, 2.0],
                  [3.0, 3.0],
                  [2.0, 3.0],
                  [3.2, 2.3]])
X_neg = np.array([[0.0, 0.0],
                  [1.0, 0.0],
                  [0.0, 1.0],
                  [0.5, 0.8]])

X = np.vstack([X_pos, X_neg])
y = np.hstack([np.ones(len(X_pos)), -np.ones(len(X_neg))])

rng = np.random.default_rng(0)
eta = 1.0
w = np.zeros(2)
b = 0.0
max_epochs = 50

def draw_points(ax):
    ax.scatter(X_pos[:,0], X_pos[:,1], marker='o', label='Positive (+1)')
    ax.scatter(X_neg[:,0], X_neg[:,1], marker='x', label='Negative (-1)')
    ax.legend(loc='best')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_title('Perceptron updates: decision boundary after each mistake')

def draw_boundary(ax, w, b, xlim, ylim):
    if abs(w[1]) > 1e-8:
        xs = np.array([xlim[0], xlim[1]])
        ys = -(w[0]/w[1]) * xs - b / w[1]
        ax.plot(xs, ys, linestyle='-')
    else:
        if abs(w[0]) > 1e-8:
            x_vert = -b / w[0]
            ax.plot([x_vert, x_vert], [ylim[0], ylim[1]], linestyle='-')

pad = 1.0
x_min, x_max = X[:,0].min() - pad, X[:,0].max() + pad
y_min, y_max = X[:,1].min() - pad, X[:,1].max() + pad

plt.figure()
ax0 = plt.gca()
ax0.set_xlim([x_min, x_max])
ax0.set_ylim([y_min, y_max])
draw_points(ax0)
plt.title('Initial data (before training)')
plt.show()

updates = 0
history = [(w.copy(), b)]

for epoch in range(max_epochs):
    mistakes_this_epoch = 0
    idx = rng.permutation(len(X))
    for i in idx:
        xi, yi = X[i], y[i]
        margin = yi * (np.dot(w, xi) + b)
        if margin <= 0:
            w = w + eta * yi * xi
            b = b + eta * yi
            updates += 1
            mistakes_this_epoch += 1
            history.append((w.copy(), b))

            plt.figure()
            ax = plt.gca()
            ax.set_xlim([x_min, x_max])
            ax.set_ylim([y_min, y_max])
            draw_points(ax)
            draw_boundary(ax, w, b, (x_min, x_max), (y_min, y_max))
            ax.set_title(f'After update #{updates}: w={w}, b={b:.2f}')
            plt.show()

    if mistakes_this_epoch == 0:
        break

print(f"Total updates until convergence: {updates}")
print(f"Final w: {w}, Final b: {b:.2f}")

# Final separator
plt.figure()
axf = plt.gca()
axf.set_xlim([x_min, x_max])
axf.set_ylim([y_min, y_max])
draw_points(axf)
draw_boundary(axf, w, b, (x_min, x_max), (y_min, y_max))
axf.set_title(f'Final separator (updates={updates})')
plt.show()